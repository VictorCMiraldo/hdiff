# Cover Letter for final version.

We would like to thank the review committee for their
valuable feedback. It greatly improved the quality of 
our paper.

## Revisions

The majot changes made are summarized by:

  * Added example oracle for 2-3-trees. [lines: ....]
  * Included discussion of related work suggested by Reviewers. [lines: ...]

It goes without saying we have processed the minor changes the reviewers
have suggested to the best of our ability.

### Detailed Revisions

Below we address the comments related to those major changes
one by one, in the order they show up in the final paper.

  C> Paper title: The paper didn't explain what makes the presented
  C> algorithms "type-directed". I understand that the algorithms consume
  C> and produce data belonging to a datatype, but I couldn't see where the
  C> algorithm is directed by the type of the tree. Wouldn't it be more
  C> precise to say that the produced patches are type-safe?

  We have changed 'type-directed' into 'type-safe' on the title
  of our paper.




  A>  2. I found the need for minimizing changes (section 2.3) is not
  A>     well-motivated, especially as it not only reduces the size of the diffs (which
  A>     is nice, but not critical), but changes the semantics of a patch. Before
  A>     minimizing. It changes a change like
  A> 
  A>         (Node2C (Node3C t1 1 2) (Node3C t2 1 2), Node2C (Node3C t3 2 1) (Node3c t4 2 1))
  A> 
  A>     where we swap subtrees in two positions, assuming they are identical, to
  A> 
  A>         (Node2C (Node3C t1 1 2) (Node3C t2 3 4), Node2C (Node3C t3 2 1) (Node3c t4 4 3))
  A> 
  A>     where we now still do the same swapping, but the subtrees do not have to be
  A>     related.
  A> 
  A>     > *Authors write*:
  A>     > Just to clarify, minimizing changes does not introduce new variables. In
  A>     > fact, the change illustrated above,
  A>     >
  A>     >     (Node2C (Node3C t1 1 2) (Node3C t2 1 2), Node2C (Node3C t3 2 1) (Node3c t4 2 1))
  A>     >
  A>     > is already in minimal form. The closure function would return this
  A>     > without further modifications.
  A> 
  A>     I used fresh names to indicate variables bound in separate `Change23`. Here
  A>     is my argument again, phrased a bit more elaborately:
  A> 
  A>     You say that
  A> 
  A>         (Node2C (Node3C t1 1 2) (Node3C t2 1 2), Node2C (Node3C t3 2 1) (Node3c t4 2 1))
  A> 
  A>     is already in minimal form, but it seems that gcp would return
  A> 
  A>         (Node2C (Node3C (Hole (t1,t3)) (Hole (1,2)) (Hole (2,1))) (Node3C (Hole (t2,t4)) (Hole (1,2)) (Hole (2,1))))
  A> 
  A>     and `closure` would then change that back to
  A> 
  A>         (Node2C (Hole (Node3C t1 1 2, Node3C t3 2 1)) (Hole (Node3C t2 1 2, Node3C t4 2 1)))
  A> 
  A>     because `(Node3C t1 1 2, Node3C t3 2 1)` is closed. But (assuming that the
  A>     variables a local to each `Change23`) that patch has a different semantics
  A>     than the original one: It applies to *more* input as before (because the
  A>     `1` in the first `Change23` no longer need to be identical to the `1` in
  A>     the second `Change23`.
  A> 
  A>     Presumably, having the patch apply to *more* inputs is a good thing, but
  A>     Section 2.3 does not contain any significant discussion of this (besides
  A>     “This keeps changes small and isolated”.) If the algorithm is the main
  A>     focus of the paper, then this deserves more discussion, in particular
  A>     stating propositions like “a minimizing a patch only extends its domain,
  A>     and does not change its effect on the domain of the original patch”.

  D> * S2.3. I do not understand why the $closure$ phase is necessary. In
  D>   the example in S2.1, $del$ computes an environment (a mapping)
  D>   between $MetaVar$ and $Tree23$. The environment is then used
  D>   globally by $ins$. It is not stated clearly how a patch is used in
  D>   S2.3, after the spine is introduced, but it seems that an
  D>   environment is computed for each local $Change23$, thus the problem
  D>   shown between l365-l370. (There is also a hint on p17, l791, "Our
  D>   changes... can be applied locally.") But why can we not use a global
  D>   environment that remembers the value of $Hole 0$?

  Added the necessary pieces to the "minimizing changes" section. As pointed 
  out by Reviewer A, there exists a corner case that deserves explanation. 
  We did not understand the remark on the original review, thanks for 
  clarifying. The need mor minimizing changes really is to make a simple
  merging algorithm possible. We have stressed this throughout the text.



  A>  3. In Section 4 I am missing a discussion of how to hash the data in the tree,
  A>     but this may be due to my limited knowledge about generic programming.
  A>     Shouldn't there be some type-class constraints somewhere that ensure that the
  A>     leafs of the data type support an `encode` function?

  C> L917: "Looking up whether a tree x is a subtree of some tree s can be
  C> done by looking up x’s topmost hash, also called the merkle root,
  C> against the trie generated from s." Explain: Why is comparing to the
  C> root only meaningful? Why is it not necessary to do a regular trie
  C> lookup?  What is the integer associated to the hash? How to make sure
  C> the two tries agree on that integer?

  C> L955: "The same subtree might appear in different places in s and d,
  C> for the Int associated with it will differ from mkSharingTrie s′ and
  C> mkSharingTrie d′." unclear

  C> L962: "When using a cryptographic hash, the chance of collision is
  C> negligible and we chose to ignore it." Citation needed.

  Expanded on the Oracle construction. Section 2.4 now implements
  an oracle for 2-3-Trees and Section 4 was demoted to Section 3.3, and only
  handles the generic construction. The 2-3-Trees oracle already serves as a 
  pedagogical example to the generic construction.



  R2> (More serious remark.) The problem addressed in Section 4 (Defining
  R2> the Oracle) seems to be solved by **hash-consing**, a well-known
  R2> technique which predates Merkle trees and which should not be
  R2> rediscovered or reexplained in an ICFP paper. See e.g. [Filliatre and
  R2> Conchon's
  R2> paper](https://www.lri.fr/~filliatr/ftp/publis/hash-consing2.pdf) for
  R2> an OCaml implementation of hash-consing. Aren't there generic,
  R2> re-usable Haskell implementations of hash-consing out there? If there
  R2> are, could they be used? If there aren't, then Section 4 should
  R2> indicate that it proposes a datatype-generic implementation of
  R2> hash-consing.

  Referenced the hash-consing work mentioned by the reviewers,
  in comment A2. It is worthwhile to mention that hash-consing is different
  than what we do. In hash-consing, one keeps a global hash table in order
  to share values as they are constructed (in memory). This allows one to
  use pointer equality instead of structural equality. Our data structure
  is different (we use Tries) to allow for faster intersection operation.
  Moreover, we are not interested in sharing values in memory, we are interested
  in knowing which trees are shared among two fixed trees. 



  R2> On the same theme, it may be worth noting that minimizing acyclic
  R2> deterministic finite automata (ADFAs) can be done in linear time; see
  R2> the papers by Revuz (1992) or Bubenzer (2011). So the problem of
  R2> "defining an oracle" has efficient solutions in the literature, which
  R2> should be cited.

  Referenced minimizing acyclic determinisitc finite automata.



  A>  5. Re evaluation: What constitutes a “conflict” (line 1071). Conflicts are not
  A>     a native concept in Git... so presumably you looked at all merge commits,
  A>     and tried to recreate the merge uthem sing a naive `git merge` command, and
  A>     checked if that succeeds? Please be more precise there. (You hint at
  A>     `diff3` in line 1133, but more details would be welcome.)

  C> L1068: The authors don't discuss the performance requirements for
  C> diffing algorithms in practice. A ballpark comparison to textual diff
  C> tools would greatly help put the performance numbers into context.

  Added more details about our experiments, particularly about 
  what we consider a merge.



  A>     BTW, did you know that even when a Pull Request is merged using rebase or
  A>     squash on GitHub, the `refs/pulls/123/head` reference (i.e. a hidden
  A>     branch) is still available and refers to the state before the rebase/merge.
  A>     You might find more interesting merge commits on these branches.

  Added another threat to validity on how we could have used Reviewer's A
  suggestion to gather more data.



  B> I do not intend to argue that the proposed algorithm is the same as
  B> grammar-based compression. My intention is that the proposed
  B> algorithm can be understood as a variant of grammar-based compression,
  B> and from this viewpoint, the proposed method leaves some room for
  B> reducing the size of patch.

  C> Related work: The authors should relate their work to incremental
  C> parsers and how these represent changed trees (see for example the
  C> work by Wagner and Graham).

  Discussed the relation of our patches to Grammar-Based compression and how this
  leaves room for reducing the patch size. We'd like to thank the clarification
  made by Reviewer B. We have also added related work to incremental parsing.


[END OF THE LETTER]
